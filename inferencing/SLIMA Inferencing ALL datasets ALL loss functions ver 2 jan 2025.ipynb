{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44259999-d661-4674-9382-76867763d7a7",
   "metadata": {},
   "source": [
    "INFERENCING WITH ALL DATASETS AND ALL LOSS FUNCTIONS, WITH DIFFERENT AUGMENTATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3cff3-ec4d-476e-a2f9-94bbd282cb0b",
   "metadata": {},
   "source": [
    "Summary of the Code:\n",
    "Dataset Extraction: All datasets (LFW, CFP, JAFFE, CALFW, CPLFW) are extracted and loaded.\n",
    "Augmentations: Applied augmentations like lighting variations, pose changes, and distortions to simulate challenging conditions.\n",
    "Model Loading: Models trained with different loss functions (FuzzyArcFace, ArcFace, AdaptiveFace, etc.) are loaded for inference.\n",
    "Embedding Extraction: For each dataset, embeddings are extracted from the original and augmented images.\n",
    "Cosine Similarity Calculation: Cosine similarity between original and augmented embeddings is computed for each model.\n",
    "Saving Results: Results are stored in a CSV file for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcea673-5f77-438c-a8f9-36493a8a2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import tarfile\n",
    "import zipfile\n",
    "#import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4b7a74-d28b-428d-a94a-5564ce5aaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3f68e2-96ff-4b2a-aa3d-52d36e3048aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bed6b9f-901b-46c3-bd90-be9b728d4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import kornia.augmentation as K\n",
    "import kornia.filters as KF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a255d4d0-ddf5-4766-9483-de10f698fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4481b95a-b4ea-4659-b512-92c130278356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012ff313-1f6e-487f-af3d-58d1ed0ba460",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b46545-a996-4fe3-9809-937421c2171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08aa3a8-7603-4b22-abf2-c90600b1c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from torch.nn import DataParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c71799-88f1-4465-8702-65e7808eeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9308f2f7-e730-43d6-8187-8e81173c34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443b6826-575a-4caf-85a4-ceb26c0fd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0a56f5-f7a1-474a-bd82-b68a5dde5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet101, ResNet101_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5232139d-8c6a-456f-a3c9-c8aa55eab712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c20c8d-3cb8-4526-bb9a-da814fdd265e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from PIL import ImageFilter\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e35d9e1-c6df-4989-8307-3a59084a25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "710b8c86-d9f4-4699-9fb1-7b1be101b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable semaphore tracking to avoid resource tracker warnings\n",
    "from multiprocessing.resource_tracker import ResourceTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "330d1bc4-fa43-49e9-9890-d233b608c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc6bbb8f-f189-4829-85a7-8b0b0f6fc44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing as mp\n",
    "def _fix_semaphore_tracker():\n",
    "    ResourceTracker._CLEANUP_FUNCS.pop(\"semaphore\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b75415b-346d-4f18-bb1f-3dc4ae6ede35",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/rapids/notebooks/slima/DATABASES'\n",
    "model_save_path = '/home/rapids/notebooks/slima'\n",
    "subdirectory = 'pth'\n",
    "\n",
    "# Set output directory\n",
    "output_dir = '/home/rapids/notebooks/slima'\n",
    "\n",
    "#num classes lfw\n",
    "num_classes=5749\n",
    "batchsize=32\n",
    "numworkers=4\n",
    "percentilevalue=10\n",
    "#thresholdtype=\"percentile\"\n",
    "thresholdtype=\"fixed\"\n",
    "depthextraction=0\n",
    "subsetratio=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8602498e-cdb9-44cf-8edf-43fac5b3d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eb14dd6-2db6-4100-8e79-7ff70dea31c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 GPUs available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"{torch.cuda.device_count()} GPUs available\")\n",
    "else:\n",
    "    print(\"Single GPU or no GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25fb8d8a-8df9-4eec-8a67-0d03836ff169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6e734e-9679-4512-b6f5-720216422e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7b882f1-9071-4308-8e93-776999a7ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If using CUDA (PyTorch)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if you are using multi-GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80db2803-8cbe-4a6e-ba3a-f3f8e5c9a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('forkserver', force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "184e2607-8bfc-42f5-8a82-dfb4e9b19869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_fix_semaphore_tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5e7220b-75ac-4ce2-b9fe-43d3ef90ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Set up available GPUs and seed for reproducibility\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c34612c-360a-4157-8390-1ce30ec390e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='processing_log42.log', level=logging.INFO, \n",
    "                    format='%(asctime)s %(message)s', filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ad9fcfc-89a2-4e96-8f7a-598fc9e31d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(dataset_name, progress_percentage, images_processed, total_images):\n",
    "    logging.info(f\"{dataset_name}: {progress_percentage:.2f}% complete ({images_processed}/{total_images} images)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d13acbc1-d3b5-4525-a10d-4235cb7088f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----------------- Data Extraction -------------------\n",
    "\n",
    "# Paths to the archives and where to extract them\n",
    "lfw_tgz_path = os.path.join(base_path, 'lfw.tgz')\n",
    "lfw_extract_path = os.path.join(base_path, 'extracted', 'lfw/lfw')\n",
    "\n",
    "cfp_tar_path = os.path.join(base_path, 'CFP.tar')\n",
    "cfp_extract_path = os.path.join(base_path, 'extracted', 'Data/Images')\n",
    "\n",
    "jaffedbase_tar_path = os.path.join(base_path, 'jaffedbase.tar')\n",
    "jaffedbase_extract_path = os.path.join(base_path, 'extracted', 'jaffedbase/jaffedbase')\n",
    "\n",
    "calfw_zip_path = os.path.join(base_path, 'calfw.zip')\n",
    "calfw_extract_path = os.path.join(base_path, 'extracted', 'calfw/calfw')\n",
    "\n",
    "cplfw_zip_path = os.path.join(base_path, 'cplfw.zip')\n",
    "cplfw_extract_path = os.path.join(base_path, 'extracted', 'cplfw/cplfw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19478748-a076-4008-809d-06ba1f3d2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract tar files (CFP, Jaffedbase, LFW)\n",
    "for tar_path, extract_path in [(cfp_tar_path, cfp_extract_path), \n",
    "                               (jaffedbase_tar_path, jaffedbase_extract_path),\n",
    "                               (lfw_tgz_path, lfw_extract_path)]:\n",
    "    if not os.path.exists(extract_path):\n",
    "        with tarfile.open(tar_path, 'r') as tar:\n",
    "            tar.extractall(extract_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da5fc5c8-5179-4589-b3e2-ec9f770e5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract zip files (CALFW, CPLFW)\n",
    "for zip_path, extract_path in [(calfw_zip_path, calfw_extract_path), \n",
    "                               (cplfw_zip_path, cplfw_extract_path)]:\n",
    "    if not os.path.exists(extract_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cf1041f-e8da-4777-b727-0b959d8229db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatDirectoryImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images from a directory where images\n",
    "    are stored in the same folder, with class labels extracted from filenames.\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        \n",
    "        # Walk through the dataset directory and gather images and labels\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png', '.tiff')):\n",
    "                    img_path = os.path.join(root, file)\n",
    "                    self.image_paths.append(img_path)\n",
    "                    \n",
    "                    # Extract class label from the filename (before the first underscore)\n",
    "                    class_name = file.split('_')[0]\n",
    "                    \n",
    "                    # If the class hasn't been seen before, add it to class_to_idx mapping\n",
    "                    if class_name not in self.class_to_idx:\n",
    "                        self.class_to_idx[class_name] = len(self.class_to_idx)\n",
    "                    \n",
    "                    # Assign the numerical label based on class_to_idx mapping\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6ca509a-b53e-45ab-8bf6-5036c6bbf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Define the iResNet100 architecture\n",
    "class iResNet100(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes):  # LFW classes\n",
    "        super(iResNet100, self).__init__()\n",
    "        #self.model = models.resnet101(pretrained=True)\n",
    "        self.model = resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e55aeb7-be0f-497c-b934-6c636ffddd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUAugmentations(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that applies multiple GPU-accelerated augmentations to a batch of images.\n",
    "    Images are expected to be tensors of shape (B, C, H, W) on CUDA.\n",
    "    \"\"\"\n",
    "    def __init__(self, brightness=0.5, rotation_degrees=30):\n",
    "        super().__init__()\n",
    "        self.augmentations = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            K.ColorJitter(brightness=brightness),\n",
    "            K.RandomRotation(degrees=rotation_degrees)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        return self.augmentations(x)\n",
    "\n",
    "class RandomOcclusionGPU(nn.Module):\n",
    "    \"\"\"\n",
    "    Randomly occludes portions of the image by setting random patches to zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_holes=3, max_hole_size=(30, 30)):\n",
    "        super().__init__()\n",
    "        self.max_holes = max_holes\n",
    "        self.max_hole_size = max_hole_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        for i in range(B):\n",
    "            for _ in range(self.max_holes):\n",
    "                hole_w = random.randint(1, self.max_hole_size[0])\n",
    "                hole_h = random.randint(1, self.max_hole_size[1])\n",
    "                top = random.randint(0, H - hole_h)\n",
    "                left = random.randint(0, W - hole_w)\n",
    "                x[i, :, top:top+hole_h, left:left+hole_w] = 0\n",
    "        return x\n",
    "\n",
    "class AddGaussianNoiseGPU(nn.Module):\n",
    "    def __init__(self, mean=0., std=25.):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        noise = torch.normal(mean=self.mean, std=self.std, size=x.size(), device=x.device)\n",
    "        # Ensure noise is scaled to [-1,1] if working with normalized images\n",
    "        noise = noise / 255.0\n",
    "        return torch.clamp(x + noise, -1, 1)  # Assuming images normalized to [-1,1]\n",
    "\n",
    "class CompressImageGPU(nn.Module):\n",
    "    \"\"\"\n",
    "    Approximate JPEG compression artifacts on GPU by:\n",
    "    - Downsampling and upsampling the image (simulating loss of detail).\n",
    "    - Optionally adding some block noise.\n",
    "    This is a heuristic, not a true JPEG compression.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factors=[0.5, 0.75], block_noise_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.scale_factors = scale_factors\n",
    "        self.block_noise_prob = block_noise_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # Randomly choose a scale factor to simulate compression\n",
    "        scale_factor = random.choice(self.scale_factors)\n",
    "        new_H = int(H * scale_factor)\n",
    "        new_W = int(W * scale_factor)\n",
    "        \n",
    "        # Downsample\n",
    "        x_ds = F.interpolate(x, size=(new_H, new_W), mode='bilinear', align_corners=False)\n",
    "        # Upsample back\n",
    "        x_rec = F.interpolate(x_ds, size=(H, W), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Optionally add block noise\n",
    "        if random.random() < self.block_noise_prob:\n",
    "            # Add block patterns\n",
    "            block_size = 8\n",
    "            for i in range(B):\n",
    "                # Create block noise pattern\n",
    "                for row in range(0, H, block_size):\n",
    "                    for col in range(0, W, block_size):\n",
    "                        # Slight random shift for block value\n",
    "                        noise_val = (torch.rand(1, device=x.device)-0.5)/50\n",
    "                        x_rec[i, :, row:row+block_size, col:col+block_size] += noise_val\n",
    "            x_rec = torch.clamp(x_rec, -1, 1)\n",
    "\n",
    "        return x_rec\n",
    "\n",
    "class ObfuscateImageGPU(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Gaussian blur multiple times with increasing radius to simulate different levels of obfuscation.\n",
    "    Kornia's gaussian_blur2d expects kernel sizes and sigma. We simulate increasing blur by increasing sigma.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_blur_radius=6, steps=6):\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "        self.sigmas = [((i+1)*(max_blur_radius/self.steps)) for i in range(self.steps)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Returns a list of blurred versions, but to integrate with the original code,\n",
    "        # we'll return them as a list of tensors. The calling code can stack them if needed.\n",
    "        B, C, H, W = x.shape\n",
    "        blurred_images = []\n",
    "        for sigma in self.sigmas:\n",
    "            # Kernel size approx: kernel_size ~ (sigma * 3)\n",
    "            k = max(3, int(sigma*3)//2*2+1)  # Ensure odd kernel size\n",
    "            blurred = KF.gaussian_blur2d(x, (k, k), (sigma, sigma))\n",
    "            blurred_images.append(blurred)\n",
    "        return blurred_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e99e18-e5ec-4816-b3b5-440c8de19c7f",
   "metadata": {},
   "source": [
    "1. Base Augmentation\n",
    "Key: \"Base\"\n",
    "Description: Applies general augmentations (e.g., random horizontal flips, brightness adjustments, random rotations). These modifications are mild and retain most of the original image characteristics.\n",
    "Distortion Level: Minimal.\n",
    "Purpose: Provides baseline augmentation to make the model robust to small variations.\n",
    "2. Obfuscation\n",
    "Keys: \"Obfuscation_1\", \"Obfuscation_2\"\n",
    "Description: Applies Gaussian blur with varying levels of intensity.\n",
    "\"Obfuscation_1\": Blurs the image with a lower blur radius (max_blur_radius=6, steps=6).\n",
    "\"Obfuscation_2\": Applies more intense blurring (max_blur_radius=12, steps=12).\n",
    "Distortion Level:\n",
    "\"Obfuscation_1\": Mild to moderate distortion.\n",
    "\"Obfuscation_2\": Higher distortion as details become less visible.\n",
    "Purpose: Simulates scenarios where images are blurry, such as low-quality scans or motion blur.\n",
    "3. Gaussian Noise\n",
    "Keys: \"Noise_1\", \"Noise_2\", \"Noise_3\"\n",
    "Description: Adds random Gaussian noise to images with varying intensity.\n",
    "\"Noise_1\": Low noise level (std=5.0).\n",
    "\"Noise_2\": Moderate noise level (std=25.0).\n",
    "\"Noise_3\": High noise level (std=50.0).\n",
    "Distortion Level:\n",
    "\"Noise_1\": Minimal distortion.\n",
    "\"Noise_2\": Moderate distortion.\n",
    "\"Noise_3\": High distortion; fine details are overwhelmed by noise.\n",
    "Purpose: Simulates noisy environments, such as images captured in poor lighting or using low-quality sensors.\n",
    "4. Occlusion\n",
    "Keys: \"Occlusion_1\", \"Occlusion_2\", \"Occlusion_3\", \"Occlusion_4\"\n",
    "Description: Randomly occludes portions of the image by setting rectangular patches to zero (blackout).\n",
    "\"Occlusion_1\": Small occlusion (max_hole_size=(30, 30)).\n",
    "\"Occlusion_2\": Medium occlusion (max_hole_size=(60, 60)).\n",
    "\"Occlusion_3\": Large occlusion (max_hole_size=(90, 90)).\n",
    "\"Occlusion_4\": Maximum occlusion (max_hole_size=(112, 112)), equivalent to occluding the entire image.\n",
    "Distortion Level:\n",
    "\"Occlusion_1\": Low distortion; the occlusion covers a small area.\n",
    "\"Occlusion_4\": Very high distortion; significant image information is lost.\n",
    "Purpose: Simulates scenarios where parts of the image are obscured (e.g., objects blocking the view).\n",
    "5. Compression\n",
    "Keys: \"Compression_1\", \"Compression_2\"\n",
    "Description: Simulates image compression artifacts by downsampling and upsampling with varying scales.\n",
    "\"Compression_1\": High compression (scale_factors=[0.1, 0.5]), leading to significant loss of detail.\n",
    "\"Compression_2\": Lower compression (scale_factors=[0.5, 0.75]), retaining more details.\n",
    "Distortion Level:\n",
    "\"Compression_1\": High distortion due to aggressive compression.\n",
    "\"Compression_2\": Moderate distortion.\n",
    "Purpose: Simulates low-quality image compression, such as JPEG artifacts or images resized to lower resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "284dc625-c437-4ea1-9250-9343b5a36a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_transformations_gpu(images=None, return_keys_only=False):\n",
    "    \"\"\"\n",
    "    Returns augmented image batches with labels identifying each augmentation\n",
    "    or just the augmentation keys if `return_keys_only` is True.\n",
    "    \"\"\"\n",
    "    augmentations = {\n",
    "        \"Base\": GPUAugmentations()(images) if images is not None else None,\n",
    "        \"Obfuscation_1\": ObfuscateImageGPU(max_blur_radius=6, steps=6)(images)[0] if images is not None else None,\n",
    "        \"Obfuscation_2\": ObfuscateImageGPU(max_blur_radius=12, steps=12)(images)[1] if images is not None else None,\n",
    "        \"Noise_1\": AddGaussianNoiseGPU(mean=0., std=5.)(images) if images is not None else None,\n",
    "        \"Noise_2\": AddGaussianNoiseGPU(mean=0., std=25.)(images) if images is not None else None,\n",
    "        \"Noise_3\": AddGaussianNoiseGPU(mean=0., std=50.)(images) if images is not None else None,\n",
    "        \"Occlusion_1\": RandomOcclusionGPU(max_holes=1, max_hole_size=(30, 30))(images.clone()) if images is not None else None,\n",
    "        \"Occlusion_2\": RandomOcclusionGPU(max_holes=1, max_hole_size=(60, 60))(images.clone()) if images is not None else None,\n",
    "        \"Occlusion_3\": RandomOcclusionGPU(max_holes=1, max_hole_size=(90, 90))(images.clone()) if images is not None else None,\n",
    "        \"Occlusion_4\": RandomOcclusionGPU(max_holes=1, max_hole_size=(112, 112))(images.clone()) if images is not None else None,\n",
    "        \"Compression_1\": CompressImageGPU(scale_factors=[0.1, 0.5], block_noise_prob=0.5)(images) if images is not None else None,\n",
    "        \"Compression_2\": CompressImageGPU(scale_factors=[0.5, 0.75], block_noise_prob=0.5)(images) if images is not None else None,\n",
    "    }\n",
    "    return list(augmentations.keys()) if return_keys_only else augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f31d6554-65d5-47dc-8461-120af9a43a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_augmentation_keys = apply_all_transformations_gpu(return_keys_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "079a2eba-93c1-4726-8232-ea9a2e34c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Transformations: 12\n"
     ]
    }
   ],
   "source": [
    "num_transformations = len(apply_all_transformations_gpu(return_keys_only=True))\n",
    "print(f\"Number of Transformations: {num_transformations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab48314f-06ab-4bf1-b05c-71376fe654cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for balancing out augmentations and negative sampled images per batch\n",
    "sample_pairs_negative=batchsize*num_transformations  #max value\n",
    "sample_pairs_positive=batchsize   #max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b713d21a-3bc2-433c-be56-9b2b6de77a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_module_prefix(state_dict):\n",
    "    \"\"\"Remove 'module.' prefix from keys in state_dict if model was trained with DataParallel.\"\"\"\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k[7:] if k.startswith(\"module.\") else k  # Remove 'module.' prefix if present\n",
    "        new_state_dict[new_key] = v\n",
    "    return new_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "399bd877-b829-46e9-bfd1-2669353e5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, rank):\n",
    "    \"\"\"\n",
    "    Load a pre-initialized model into the appropriate GPU and wrap in DDP.\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "    model = model.to(device)  # Move model to the correct rank-specific GPU\n",
    "\n",
    "    # Wrap the model in DistributedDataParallel\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d207d3bb-91da-48f6-b0e8-fd7d0173658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model paths\n",
    "model_filenames = [\n",
    "    'FuzzyArcFace_2024-10-27.pth', \n",
    "    'FuzzyArcFace_tau 0.1.pth', \n",
    "    'FuzzyArcFace_tau 0.5.pth', \n",
    "    'ArcFace_2024-10-28.pth', \n",
    "    'AdaptiveFace_2024-10-29.pth',\n",
    "    'VPL_2024-10-29.pth', \n",
    "    'SphereFace2_2024-10-30.pth', \n",
    "    'UniFace_2024-10-31.pth'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b790f64-9c38-4d37-9ba6-365f26b57796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names in the same order as model_filenames\n",
    "model_names = [\"FuzzyArcFace9\", \n",
    "               \"FuzzyArcFace5\",\n",
    "               \"FuzzyArcFace1\",\n",
    "               \"ArcFace\", \n",
    "               \"AdaptiveFace\", \n",
    "               \"VPL\", \n",
    "               \"SphereFace2\", \n",
    "               \"UniFace\"\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5c36ddf-1707-41e5-a9f7-8a674131e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33268aa8-90af-4b1e-9868-82bea98be9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract embeddings\n",
    "def extract_embedding(model, image, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Check if the input is already batched (4D)\n",
    "    if image.dim() == 3:  # If the image is 3D, add the batch dimension\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    image = image.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(image)  # No need to unsqueeze again here\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "060f5787-795d-4d24-b197-f15260d16d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_loader(dataloader, subset_ratio=subsetratio, batch_size=batchsize):\n",
    "    \"\"\"\n",
    "    Get a DataLoader with a random subset specific to the rank.\n",
    "    \"\"\"\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "    subset_size = max(1, int(subset_ratio * dataset_size))  # Ensure at least one sample\n",
    "\n",
    "    # Use DistributedSampler for rank-specific subsets\n",
    "    sampler = DistributedSampler(dataloader.dataset, num_replicas=dataloader.sampler.num_replicas, rank=dataloader.sampler.rank)\n",
    "    subset_indices = list(iter(sampler))[:subset_size]  # Get only the rank-specific subset\n",
    "    subset_dataset = Subset(dataloader.dataset, subset_indices)\n",
    "\n",
    "    return DataLoader(subset_dataset, batch_size=batch_size, shuffle=False, num_workers=numworkers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcd79a01-51a5-42f3-8bbe-6506eb6ed00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, images, device):\n",
    "    \"\"\"\n",
    "    Process a batch of images and return embeddings for the original images and their augmentations.\n",
    "    \n",
    "    Arguments:\n",
    "    - model: The model used for extracting embeddings.\n",
    "    - images: A batch of images (tensor).\n",
    "    - device: Device on which to run the model (e.g., 'cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    - original_embeddings: A dictionary with original embeddings for each image in the batch.\n",
    "    - augmented_embeddings: A dictionary with augmented embeddings for each image in the batch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()  # Start timing\n",
    "   \n",
    "\n",
    "    original_embeddings = {}\n",
    "    augmented_embeddings = {}\n",
    "\n",
    "    # images is already a batch of shape (N, C, H, W)\n",
    "    # Ensure images are on GPU\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract original embeddings\n",
    "        original_batch_embedding = model(images)  # (N, embedding_dim)\n",
    "\n",
    "    # Process each image in the batch\n",
    "    for i in range(images.size(0)):\n",
    "        # Store original embeddings in a dictionary with keys as indices\n",
    "        print(f\"Processing image {i + 1} of {images.size(0)}\")\n",
    "        original_embeddings[i] = original_batch_embedding[i].cpu()\n",
    "        \n",
    "        # Apply augmentations in parallel (runs on CPU threads)\n",
    "        aug_start_time = time.time()\n",
    "        all_augs = apply_all_transformations_gpu(images)\n",
    "        \n",
    "        # Weâ€™ll process all augmentations in batches for efficiency:\n",
    "        #augmented_embeddings = {i: [] for i in range(images.size(0))}\n",
    "        augmented_embeddings = {i: {} for i in range(images.size(0))}\n",
    "        \n",
    "  \n",
    "        with torch.no_grad():\n",
    "        # For each augmented variant, run through the model\n",
    "            for aug_type, aug_batch in all_augs.items():\n",
    "                aug_emb_batch = model(aug_batch)  # (N, embedding_dim)\n",
    "                aug_emb_batch = aug_emb_batch.cpu()\n",
    "                # Append each image's augmented embedding\n",
    "                for i in range(images.size(0)):\n",
    "                    if aug_type not in augmented_embeddings[i]:\n",
    "                        augmented_embeddings[i][aug_type] = []  # Initialize as a list\n",
    "                    augmented_embeddings[i][aug_type].append(aug_emb_batch[i])\n",
    "                    #print(f\"Augmentations took {time.time() - aug_start_time:.2f} seconds\")\n",
    "            \n",
    "    print(f\"Total batch processing took {time.time() - start_time:.2f} seconds\")\n",
    "    return original_embeddings, augmented_embeddings\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e9e1bf0-30df-4f70-a801-92803959eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(images, labels):\n",
    "    \"\"\"\n",
    "    Generate positive and negative pairs from a batch of images and labels.\n",
    "    \"\"\"\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] == labels[j]:  # Positive pair\n",
    "                positive_pairs.append((i, j, 1))\n",
    "            else:  # Negative pair\n",
    "                negative_pairs.append((i, j, 0))\n",
    "\n",
    "    return positive_pairs, negative_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16ff2b94-1316-419c-99e5-a354b85bc70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_dataset(\n",
    "    dataloader,\n",
    "    models_to_test,\n",
    "    output_dir,\n",
    "    dataset_name,\n",
    "    loss_function_name,  # New parameter for loss function\n",
    "    subset_ratio=subsetratio,\n",
    "    batch_size=batchsize,\n",
    "    threshold_type=thresholdtype,\n",
    "    percentile_value=percentilevalue,\n",
    "    rank=0,\n",
    "    device=device\n",
    "):\n",
    "\n",
    "    results_df = [pd.DataFrame() for _ in range(rank+1)]  # One DataFrame for each rank\n",
    "    # Initialize dictionaries for embeddings storage and metrics calculation\n",
    "    all_original_embeddings = {}\n",
    "    all_augmented_embeddings = {}\n",
    "    similarities = []\n",
    "    true_labels = []\n",
    "    total_images = len(dataloader.dataset)  # Use dataloader instead of subset_loader\n",
    "    images_processed = 0\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    # Initialize augmented_metrics globally\n",
    "    augmented_metrics = {aug_type: {\"similarities\": [], \"true_labels\": []} for aug_type in expected_augmentation_keys}\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        print(f\"Rank {rank}: Batch {batch_idx} - Image shape: {images.shape}, Labels: {labels}\")\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Rank {rank}: Processing batch {batch_idx} on device {images.device}\")\n",
    "        for model_name, model in models_to_test.items():\n",
    "            # Add short name for debugging\n",
    "            model_display_name = getattr(model, \"short_name\", model_name)  # Use short_name if available\n",
    "            print(f\"Processing batch with model: {model_display_name}\")\n",
    "            assert all(p.device == device for p in model.parameters()), f\"Model {model_display_name} has parameters on the wrong device!\"\n",
    "              # Initialize original_embedding in case of errors in the batch\n",
    "            original_embedding = {}\n",
    "            try:\n",
    "                #generate augmented images per each original image in the batch\n",
    "                original_embedding, augmented_embedding = process_batch(model, images, device)\n",
    "\n",
    "                if len(original_embedding) != len(labels):\n",
    "                    print(f\"Warning: original_embedding length ({len(original_embedding)}) does not match labels length ({len(labels)})\")\n",
    "                    continue\n",
    "\n",
    "                # Compare augmented images against its original image. True labels are true always \n",
    "                for i in range(len(original_embedding)):\n",
    "                    emb_i = extract_tensor_from_nested_dict(original_embedding[i])\n",
    "                    for aug_type, augmented_emb in augmented_embedding[i].items():\n",
    "                        emb_k = extract_tensor_from_nested_dict(augmented_emb)\n",
    "                        if emb_k.dim() > 1:\n",
    "                            emb_k = emb_k.flatten()  # Ensure it's a 1D vector\n",
    "                        similarity = F.cosine_similarity(emb_i.unsqueeze(0), emb_k.unsqueeze(0)).item()\n",
    "                        similarities.append(similarity)\n",
    "                        true_labels.append(1)  # Positive pairs\n",
    "                        # Append to augmentation-specific metrics\n",
    "                        augmented_metrics[aug_type][\"similarities\"].append(similarity)\n",
    "                        augmented_metrics[aug_type][\"true_labels\"].append(1)\n",
    "\n",
    "                # Generate positive and negative pairs\n",
    "                positive_pairs, negative_pairs = generate_pairs(images, labels)\n",
    "                \n",
    "\n",
    "                # Limit the number of pairs to 4 each (adjust if needed)\n",
    "                num_positive_samples = min(sample_pairs_positive, len(positive_pairs)) #in general positive samples are less than negative\n",
    "                num_negative_samples = min(sample_pairs_negative, len(negative_pairs))\n",
    "                print(f\"AFTER Num positive samples...{num_positive_samples}, Num negative samples...{num_negative_samples}\")\n",
    "\n",
    "                # Randomly sample pairs\n",
    "                sampled_positive_pairs = random.sample(positive_pairs, num_positive_samples) if positive_pairs else []\n",
    "                sampled_negative_pairs = random.sample(negative_pairs, num_negative_samples) if negative_pairs else []\n",
    "                print(f\"Sampled positive pairs...{sampled_positive_pairs}, Sampled negative pairs...{sampled_negative_pairs}\")\n",
    "                \n",
    "                # Compute similarities for negative pairs. True labels are false always\n",
    "                for i, j, label in sampled_negative_pairs:\n",
    "                    emb_i = extract_tensor_from_nested_dict(original_embedding[i])\n",
    "                    emb_j = extract_tensor_from_nested_dict(original_embedding[j])\n",
    "    \n",
    "                    if emb_i is None or emb_j is None:\n",
    "                        print(f\"Skipping comparison for indices {i}, {j} due to incompatible types.\")\n",
    "                        continue\n",
    "    \n",
    "                    if emb_i.dim() > 1:\n",
    "                        emb_i = emb_i.flatten()\n",
    "                    if emb_j.dim() > 1:\n",
    "                        emb_j = emb_j.flatten()\n",
    "    \n",
    "                    similarity = F.cosine_similarity(emb_i.unsqueeze(0), emb_j.unsqueeze(0)).item()\n",
    "                    similarities.append(similarity)\n",
    "                    true_labels.append(0)\n",
    "\n",
    "                # Compute similarities for positive pairs. True labels are true always, if there are positive samples.\n",
    "                for i, j, label in sampled_positive_pairs:\n",
    "                    emb_i = extract_tensor_from_nested_dict(original_embedding[i])\n",
    "                    emb_j = extract_tensor_from_nested_dict(original_embedding[j])\n",
    "\n",
    "    \n",
    "                    if emb_i is None or emb_j is None:\n",
    "                        print(f\"Skipping comparison for indices {i}, {j} due to incompatible types.\")\n",
    "                        continue\n",
    "    \n",
    "                    if emb_i.dim() > 1:\n",
    "                        emb_i = emb_i.flatten()\n",
    "                    if emb_j.dim() > 1:\n",
    "                        emb_j = emb_j.flatten()\n",
    "\n",
    "                    #similarity for positive pair\n",
    "                    similarity = F.cosine_similarity(emb_i.unsqueeze(0), emb_j.unsqueeze(0)).item()\n",
    "                    similarities.append(similarity)\n",
    "                    true_labels.append(1)\n",
    "            except Exception as exc:\n",
    "                print(f\"Error processing batch {batch_idx} for model {model_name}: {exc}\")\n",
    "                \n",
    "        # Store batch results for this dataset, ensuring original_embedding is available and consistent\n",
    "        if original_embedding and len(original_embedding) == len(labels):  # Add this condition\n",
    "            batch_results = {\n",
    "                'Index': list(original_embedding.keys()),\n",
    "                'True Label': labels.cpu().numpy(),\n",
    "                'Embedding': [embedding.numpy() for embedding in original_embedding.values()]\n",
    "            }\n",
    "\n",
    "            batch_df = pd.DataFrame(batch_results)\n",
    "            #results_df = pd.concat([results_df, batch_df], ignore_index=True)\n",
    "            results_df[rank] = pd.concat([results_df[rank], batch_df], ignore_index=True)\n",
    "\n",
    "            # intermediate_output_path = f\"{output_dir}/{dataset_name}_{model_name}_partial_results17.csv\"\n",
    "            intermediate_output_path = f\"{output_dir}/{dataset_name}_{model_display_name}_partial_results42_rank{rank}.csv\"\n",
    "            results_df[rank].to_csv(intermediate_output_path, mode='w', header=True, index=False)\n",
    " \n",
    "            logging.info(f\"Intermediate results for dataset '{dataset_name}' (rank {rank}) saved to {intermediate_output_path}.\")\n",
    "            logging.info(f\"Rank {rank}: Processed {len(results_df[rank])} rows in total\")\n",
    "        else:\n",
    "            print(f\"No valid embeddings for batch {batch_idx} or mismatch in original_embedding and labels.\")\n",
    "\n",
    "        images_processed += batch_size\n",
    "        progress_percentage = (images_processed / total_images) * 100\n",
    "        log_progress(dataset_name, progress_percentage, images_processed, total_images)\n",
    "\n",
    "\n",
    "    #if similarities.size == 0:\n",
    "    if len(similarities) == 0:\n",
    "        raise ValueError(\"Similarities array is empty. Ensure data pipeline is functioning correctly.\")\n",
    "\n",
    "    # Calculate metrics based on similarities and true labels\n",
    "    if threshold_type == \"percentile\":\n",
    "        threshold = np.percentile(similarities, percentile_value)\n",
    "        print(f\"Threshold used for predictions: {threshold}\")\n",
    "    elif threshold_type == \"mean\":\n",
    "        threshold = np.mean(similarities)\n",
    "    elif threshold_type == \"median\":\n",
    "        threshold = np.median(similarities)\n",
    "    elif threshold_type == \"fixed\":\n",
    "        threshold = 0.5\n",
    "    else:\n",
    "        threshold = 0.5  # Default threshold if none is specified\n",
    "\n",
    "    predictions = (np.array(similarities) >= threshold).astype(int)\n",
    "    accuracy = np.mean(predictions == np.array(true_labels))\n",
    "\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for idx in range(len(similarities)):\n",
    "        print(f\"Similarity: {similarities[idx]:.4f}, True Label: {true_labels[idx]}, Prediction: {predictions[idx]}, {'Correct' if predictions[idx] == true_labels[idx] else 'Incorrect'}\")\n",
    "\n",
    "    print(f\"\\nTotal comparisons: {len(similarities)}\")\n",
    "    print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nOVERALL Metrics per dataset and rank: {loss_function_name}, Dataset: {dataset_name}, Rank: {rank}:\")\n",
    "    # Proportion of similarities above threshold\n",
    "    proportion_above_threshold = np.sum(np.array(similarities) >= threshold) / len(similarities)\n",
    "    # Recall\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    # Precision\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    # F1 Score\n",
    "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "    # Mean similarity\n",
    "    mean_similarity = np.mean(similarities)\n",
    "\n",
    "    # Calculate ROC-AUC\n",
    "    if len(np.unique(true_labels)) > 1:\n",
    "        fpr, tpr, _ = roc_curve(true_labels, similarities)\n",
    "        roc_auc = roc_auc_score(true_labels, similarities)\n",
    "    else:\n",
    "        fpr, tpr, roc_auc = [], [], 0\n",
    "\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Proportion Above Threshold: {proportion_above_threshold:.4f}\")\n",
    "    print(f\"Mean Similarity: {mean_similarity:.4f}\")\n",
    "\n",
    "    print(f\"\\nMetrics per augmentation type for Loss Function: {loss_function_name}, Dataset: {dataset_name}, Rank: {rank}:\")\n",
    "    for aug_type, metrics in augmented_metrics.items():\n",
    "        if len(metrics[\"true_labels\"]) > 0:  # Ensure data exists for this augmentation\n",
    "            num_augmentations = len(metrics[\"true_labels\"])\n",
    "            aug_predictions = [1 if sim >= threshold else 0 for sim in metrics[\"similarities\"]]\n",
    "            # Calculate recall\n",
    "            aug_recall = recall_score(metrics[\"true_labels\"], aug_predictions, zero_division=0)\n",
    "            # Proportion of predictions above threshold\n",
    "            proportion_above_threshold = sum(sim >= threshold for sim in metrics[\"similarities\"]) / len(metrics[\"similarities\"])\n",
    "            # Mean similarity for additional insight\n",
    "            mean_similarity = np.mean(metrics[\"similarities\"])\n",
    "            print(f\"Augmentation: {aug_type}, Recall: {aug_recall:.4f}, Proportion Above Threshold: {proportion_above_threshold:.4f}, Mean Similarity: {mean_similarity:.4f}, Number of Augmentations: {num_augmentations}\")\n",
    "\n",
    "    # Save final results for this dataset\n",
    "    # final_output_path = f\"{output_dir}/{dataset_name}_{model_display_name}_results42.csv\"\n",
    "    # results_df[rank].to_csv(final_output_path, mode='w', header=True, index=False)\n",
    "    #logging.info(f\"Processing complete for dataset '{dataset_name}'. Results saved to {final_output_path}.\")\n",
    "    \n",
    "    return results_df[rank], accuracy, roc_auc, fpr, tpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89622c9a-d2c9-459e-ae8c-18497125de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensor_from_nested_dict(data, depth=depthextraction):\n",
    "    \"\"\"\n",
    "    Recursively extract the first tensor-compatible element from a deeply nested structure\n",
    "    and flatten it if necessary, with added debugging.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * depth  # Indentation for readability in debug output\n",
    "    #print(f\"{indent}Data type at depth {depth}: {type(data)}\")  # Print data type\n",
    "\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        #print(f\"{indent}Found tensor at depth {depth}. Shape: {data.shape}\")\n",
    "        return data.flatten()  # Flatten tensor if itâ€™s not 1-dimensional\n",
    "\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        #print(f\"{indent}Found numpy array at depth {depth}. Shape: {data.shape}\")\n",
    "        return torch.tensor(data).flatten()  # Convert numpy arrays directly and flatten\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        #print(f\"{indent}Exploring list at depth {depth}, length: {len(data)}\")\n",
    "        for item in data:\n",
    "            tensor_value = extract_tensor_from_nested_dict(item, depth + 1)\n",
    "            if tensor_value is not None:\n",
    "                return tensor_value\n",
    "\n",
    "    elif isinstance(data, dict):\n",
    "        #print(f\"{indent}Exploring dict at depth {depth}, keys: {list(data.keys())}\")\n",
    "        for key, value in data.items():\n",
    "            tensor_value = extract_tensor_from_nested_dict(value, depth + 1)\n",
    "            if tensor_value is not None:\n",
    "                return tensor_value\n",
    "\n",
    "    # Explicitly state when None is returned at a level where no tensor was found\n",
    "    print(f\"{indent}No tensor found at depth {depth} in current structure: {data}\")\n",
    "    return None  # Return None if no tensor-compatible data is found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e25e354-60f4-4b95-b3e1-1d72af97ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging (you can adjust the level as needed)\n",
    "logging.basicConfig(level=logging.INFO)  # Set to DEBUG for detailed prints, INFO for less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa041cf2-566d-4723-bd9d-ad8229d5e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_roc_curve(fpr, tpr, roc_auc, model_name):\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title(f'ROC Curve for {model_name}')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bb8c1e7-53b2-432c-bbd3-034a916ab520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(models_to_test, test_loaders, output_dir, subset_ratios, batch_size=batchsize, rank=0):\n",
    "    \"\"\"\n",
    "    Runs tests on the specified models using the provided test loaders.\n",
    "    \"\"\"\n",
    "    for test_name, test_loader in test_loaders.items():\n",
    "        print(f\"Rank {rank}: Testing on {test_name}...\")\n",
    "        sampler = test_loader.sampler\n",
    "        print(f\"Rank {rank}: Total images in dataset: {len(sampler)}\")  # Log sampler size\n",
    "        \n",
    "        # Use the subset_ratio to get a subset loader\n",
    "        subset_loader = get_subset_loader(test_loader, subset_ratio=subset_ratios.get(test_name, subsetratio), batch_size=batch_size)\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "\n",
    "        # Evaluate each model on the subset\n",
    "        for model_name, model in models_to_test.items():\n",
    "            model_display_name = getattr(model, \"short_name\", model_name)\n",
    "            print(f\"Rank {rank}: Evaluating model '{model_display_name}' on dataset '{test_name}'...\")\n",
    "\n",
    "\n",
    "            results_df, accuracy, roc_auc, fpr, tpr = process_single_dataset(\n",
    "                subset_loader,\n",
    "                {model_name: model},\n",
    "                output_dir,\n",
    "                test_name,\n",
    "                model_display_name,  # Pass loss function name\n",
    "                subset_ratio=subsetratio,\n",
    "                rank=rank,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            print(f\"Rank {rank}: Dataset: {test_name}, Model: {model_display_name}, Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "            #plot_roc_curve(fpr, tpr, roc_auc, f\"{model_name}_{test_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c36c8871-b0fb-457c-b97a-03eb9da2b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(model_filepaths, rank, world_size):\n",
    "    \"\"\"\n",
    "    Initialize and load models using DistributedDataParallel with rank-specific optimizations.\n",
    "    \"\"\"\n",
    "    models_to_test = {}\n",
    "\n",
    "    for model_name, model_path in model_filepaths.items():\n",
    "        temp_model_path = f\"{model_name}_temp.pth\"\n",
    "        \n",
    "        # Check if the temp file exists\n",
    "        if os.path.exists(temp_model_path):\n",
    "            # Load the model state from the existing temp file\n",
    "            print(f\"Rank {rank}: Found existing temp file for {model_name}. Loading model state...\")\n",
    "            model = iResNet100()\n",
    "            device = torch.device(f\"cuda:{rank}\")\n",
    "            model = model.to(device)\n",
    "            model.load_state_dict(torch.load(temp_model_path, map_location=device))\n",
    "        else:\n",
    "            if rank == 0:\n",
    "                # Load and save the model on rank 0\n",
    "                print(f\"Rank 0: Saving temp file for {model_name}...\")\n",
    "                model = iResNet100()\n",
    "                device = torch.device(f\"cuda:{rank}\")\n",
    "                model = model.to(device)\n",
    "                \n",
    "                # Load checkpoint and remove module prefix if needed\n",
    "                checkpoint = torch.load(model_path, map_location=device)\n",
    "                checkpoint = remove_module_prefix(checkpoint)\n",
    "                model.load_state_dict(checkpoint)\n",
    "\n",
    "                # Save model state dict for sharing\n",
    "                torch.save(model.state_dict(), temp_model_path)\n",
    "            \n",
    "            # Synchronize to ensure rank 0 saves the model first\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "            # Load from saved file on other ranks\n",
    "            if rank != 0:\n",
    "                print(f\"Rank {rank}: Waiting for temp file for {model_name}...\")\n",
    "                model = iResNet100()\n",
    "                device = torch.device(f\"cuda:{rank}\")\n",
    "                model = model.to(device)\n",
    "                model.load_state_dict(torch.load(temp_model_path, map_location=device))\n",
    "\n",
    "        # Wrap in DDP\n",
    "        model = DDP(model, device_ids=[rank], output_device=rank)\n",
    "        model.short_name = model_name.split(\"_\")[0]  # Add short name for debugging\n",
    "        models_to_test[model_name] = model\n",
    "\n",
    "    return models_to_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7f049-37cd-4f32-b20a-2463e5046a54",
   "metadata": {},
   "source": [
    "CALFW: Positive and negative pairs based on age.\n",
    "CPLFW: Positive pairs with different poses.\n",
    "JEFF: Different facial expressions of the same subjects.\n",
    "CFP: Frontal and profile views of the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62c2b3e8-0f03-4f03-878c-ec0ff27281b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main worker function for each GPU\n",
    "def main_worker(rank, world_size, model_filepaths, test_datasets, output_dir, subset_ratios, batch_size):\n",
    "    try:\n",
    "        print(f\"Main worker started with rank: {rank}\")\n",
    "        # Initialize the process group for distributed training\n",
    "        #dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "        dist.init_process_group(\n",
    "        backend=\"nccl\", \n",
    "        init_method=\"env://\", \n",
    "        #init_method=\"tcp://127.0.0.1:29500\",\n",
    "        rank=rank, \n",
    "        world_size=world_size, \n",
    "        #timeout=timedelta(seconds=1200)\n",
    "        #timeout=timedelta(hours=48)\n",
    "        timeout=timedelta(days=30)  # Adjust to your maximum expected runtime\n",
    "        )\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"Starting distributed processing with {world_size} ranks.\")\n",
    "\n",
    "        dist.barrier()  # Synchronize all processes\n",
    "        \n",
    "        # Set the device based on rank\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "        torch.cuda.set_device(device)\n",
    "        \n",
    "        # Initialize models on the current device\n",
    "        models_to_test = initialize_models(model_filepaths, rank, world_size)\n",
    "        \n",
    "        # Create DataLoaders with DistributedSampler for each dataset\n",
    "        dataloaders = {\n",
    "            dataset_name: DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                sampler=DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True),\n",
    "                num_workers=numworkers,\n",
    "                drop_last=True\n",
    "            )\n",
    "            for dataset_name, dataset in test_datasets.items()\n",
    "        }\n",
    "\n",
    "        # Add the print statement here to check the sampled indices\n",
    "        for dataset_name, dataset in test_datasets.items():\n",
    "            sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, seed=42, drop_last=True)\n",
    "            print(f\"Rank {rank} is sampling indices: {list(sampler)}\")\n",
    "        \n",
    "        # Run tests on the models and datasets\n",
    "        run_tests(models_to_test, dataloaders, output_dir, subset_ratios, batch_size=batch_size,rank=rank)\n",
    "    \n",
    "\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        #for semaphore in mp.Semaphore._semaphore_tracker._semaphores.values():\n",
    "            #semaphore.release()\n",
    "                # Clean up the distributed environment\n",
    "        torch.cuda.empty_cache()\n",
    "        dist.destroy_process_group()\n",
    "        #pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "edc63d70-ebaf-4caf-b663-6d21d3c14df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASTER_ADDR: 127.0.0.1\n",
      "MASTER_PORT: 29500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'main_worker' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 33\u001b[0m\n\u001b[1;32m     25\u001b[0m subset_ratios \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPF\u001b[39m\u001b[38;5;124m\"\u001b[39m: subsetratio,    \u001b[38;5;66;03m# % for CPF\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALFW\u001b[39m\u001b[38;5;124m\"\u001b[39m: subsetratio,  \u001b[38;5;66;03m# % for CALFW\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJEFF\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# % for JEFF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPLFW\u001b[39m\u001b[38;5;124m\"\u001b[39m: subsetratio  \u001b[38;5;66;03m# % for CPLFW\u001b[39;00m\n\u001b[1;32m     30\u001b[0m }\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Spawn processes for each GPU, passing `rank` to each worker\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_filepaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_ratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:282\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    276\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:229\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    223\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(tf\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    224\u001b[0m process \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m    225\u001b[0m     target\u001b[38;5;241m=\u001b[39m_wrap,\n\u001b[1;32m    226\u001b[0m     args\u001b[38;5;241m=\u001b[39m(fn, i, args, tf\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    227\u001b[0m     daemon\u001b[38;5;241m=\u001b[39mdaemon,\n\u001b[1;32m    228\u001b[0m )\n\u001b[0;32m--> 229\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m error_files\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    231\u001b[0m processes\u001b[38;5;241m.\u001b[39mappend(process)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the environment variables needed for the distributed setup\n",
    "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"  # Use localhost for single-machine setup\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"  # Use any free port, 29500 is common for PyTorch\n",
    "\n",
    "    print(f\"MASTER_ADDR: {os.environ.get('MASTER_ADDR')}\")\n",
    "    print(f\"MASTER_PORT: {os.environ.get('MASTER_PORT')}\")\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "\n",
    "  \n",
    "    world_size = torch.cuda.device_count()  # Total number of GPUs\n",
    "\n",
    "    model_filepaths = {\n",
    "        model_name: os.path.join(model_save_path, subdirectory, filename)\n",
    "        for model_name, filename in zip(model_names, model_filenames)\n",
    "    }\n",
    "\n",
    "    test_datasets = {\n",
    "    'CPLFW': FlatDirectoryImageDataset(os.path.join(cplfw_extract_path, 'aligned images'), transform),\n",
    "    'CALFW': FlatDirectoryImageDataset(os.path.join(calfw_extract_path, 'aligned images'), transform),\n",
    "    'JEFF': FlatDirectoryImageDataset(jaffedbase_extract_path, transform),\n",
    "    'CFP': ImageFolder(os.path.join(cfp_extract_path, 'Data/Images'), transform),\n",
    "    }\n",
    "\n",
    "    subset_ratios = {\n",
    "    \"CPF\": subsetratio,    # % for CPF\n",
    "    \"CALFW\": subsetratio,  # % for CALFW\n",
    "    \"JEFF\": 1,  # % for JEFF\n",
    "    \"CPLFW\": subsetratio  # % for CPLFW\n",
    "    }\n",
    "\n",
    "    # Spawn processes for each GPU, passing `rank` to each worker\n",
    "    mp.spawn(main_worker, args=(world_size, model_filepaths, test_datasets, output_dir, subset_ratios, batchsize), nprocs=world_size, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969f722-e24c-40be-9b61-01f20f18b5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
